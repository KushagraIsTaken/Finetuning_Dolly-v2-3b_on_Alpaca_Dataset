{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "We are fine-tuning our model on Alpaca Dataset (10K Rows). The below mentioned code is to load the Alpaca Dataset from [Hugging Face](https://huggingface.co/datasets/tatsu-lab/alpaca) using datasets library and then selecting 10K rows and saving them in JSON format. We have selected only 10K rows from the dataset is to reduce the dataset size due to frequent server disconnection issue in IDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from datasets) (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/u1759c6d9527c9022f99471cd5e84981/.local/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "JSON file saved to alpaca_10000_lines.json\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install the 'datasets' library\n",
    "!pip install datasets\n",
    "\n",
    "# Step 2: Import required modules and load the Alpaca dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Load the Alpaca dataset\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "\n",
    "# Step 3: Extract 10,000 rows from the dataset\n",
    "selected_data = dataset['train'].select(range(10000))\n",
    "\n",
    "# Transform the selected rows into a list of dictionaries\n",
    "data_list = [selected_data[i] for i in range(len(selected_data))]\n",
    "\n",
    "# Step 4: Write the selected rows to a JSON file\n",
    "json_file_path = 'alpaca_10000_lines.json'\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(data_list, json_file, indent=4)\n",
    "\n",
    "print(f\"JSON file saved to {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning a Pre-Trained Language Model on Custom Dataset\n",
    "\n",
    "This code demonstrates the process of fine-tuning a pre-trained language model, specifically the `databricks/dolly-v2-3b` model, on a custom dataset using Hugging Face and Intel Extension for Transformers libraries. The selected model, `databricks/dolly-v2-3b`, is a powerful language model that is part of the Dolly series, known for its ability to generate coherent and contextually relevant text.\n",
    "\n",
    "#### What This Code Does:\n",
    "1. **Imports Necessary Libraries**:\n",
    "   - The code imports essential classes and functions from Hugging Face and Intel Extension for Transformers libraries, enabling configuration and fine-tuning of the model.\n",
    "\n",
    "2. **Model Configuration**:\n",
    "   - The `ModelArguments` class is used to specify the pre-trained model to be fine-tuned (`databricks/dolly-v2-3b`).\n",
    "\n",
    "3. **Data Configuration**:\n",
    "   - The `DataArguments` class sets up the path to the custom training dataset (`alpaca_10000_lines.json`) and defines a small validation split.\n",
    "\n",
    "4. **Training Configuration**:\n",
    "   - The `TrainingArguments` class configures various training parameters such as the output directory for saving checkpoints, batch sizes, number of epochs, gradient accumulation steps, and other relevant settings. Notably, bfloat16 precision is enabled for training, if supported, to optimize performance.\n",
    "\n",
    "5. **Finetuning Configuration**:\n",
    "   - Additional fine-tuning parameters are set using the `FinetuningArguments` class.\n",
    "\n",
    "6. **Combining Configurations**:\n",
    "   - All configurations (model, data, training, and fine-tuning arguments) are combined into a single `TextGenerationFinetuningConfig` object.\n",
    "\n",
    "7. **Initiating Fine-Tuning**:\n",
    "   - The `finetune_model` function is called with the combined configuration to start the fine-tuning process.\n",
    "\n",
    "#### Why Choose `databricks/dolly-v2-3b`?\n",
    "- **Performance**: The Dolly series models, including `dolly-v2-3b`, are known for their robust performance in text generation tasks, producing coherent and contextually appropriate responses.\n",
    "- **Flexibility**: These models are versatile and can be fine-tuned for various downstream tasks, making them suitable for a wide range of applications.\n",
    "- **Pre-Training**: The `dolly-v2-3b` model has been pre-trained on extensive datasets, providing a strong foundation for further fine-tuning on specific data.\n",
    "- **Light-Weight**: `dolly-v2-3b` is trained on 3 billion parameters, unlike larger models like `Llama-2-7b-chat-hf`, which makes the training process easier and faster.\n",
    "\n",
    "#### Use Case:\n",
    "Fine-tuning `databricks/dolly-v2-3b` on the Alpaca dataset (10,000 rows) allows customization of the model to better suit specific needs, improving its performance on tasks relevant to the provided dataset. This process is crucial for adapting the general capabilities of the pre-trained model to specialized tasks, enhancing accuracy and relevance in generated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 06:26:01.813799: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-23 06:26:02.804816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-23 06:26:06.186101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/u1759c6d9527c9022f99471cd5e84981/.conda/envs/itrex/lib/python3.10/site-packages/transformers/training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "2024-06-23 06:26:09,662 - _logger.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - WARNING - Process rank: 0, device: cpu\n",
      "distributed training: True, 16-bits training: True\n",
      "2024-06-23 06:26:09,665 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=False,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./tmp/runs/Jun23_06-26-00_idc-training-gpu-compute-05,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=True,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./tmp,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./tmp,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=True,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "/home/u1759c6d9527c9022f99471cd5e84981/.conda/envs/itrex/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47ce7fe854d4197ad632aa885b2f552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-06-23 06:26:10,869 >> loading configuration file config.json from cache at /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/hub/models--databricks--dolly-v2-3b/snapshots/f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-23 06:26:10,879 >> Model config GPTNeoXConfig {\n",
      "  \"_name_or_path\": \"databricks/dolly-v2-3b\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoXForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"custom_pipelines\": {\n",
      "    \"text-generation\": {\n",
      "      \"impl\": \"instruct_pipeline.InstructionTextGenerationPipeline\",\n",
      "      \"pt\": \"AutoModelForCausalLM\",\n",
      "      \"tf\": \"TFAutoModelForCausalLM\"\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10240,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neox\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 0.25,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_parallel_residual\": true,\n",
      "  \"vocab_size\": 50280\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc633735c73547858da4ddc4b2232230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b65b606b2634624bd8dee340c4b22d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54069a5b6ba749b9b1f39f33b08b5b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2108] 2024-06-23 06:26:12,496 >> loading file vocab.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-23 06:26:12,497 >> loading file merges.txt from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-23 06:26:12,498 >> loading file tokenizer.json from cache at /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/hub/models--databricks--dolly-v2-3b/snapshots/f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-23 06:26:12,499 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-23 06:26:12,500 >> loading file special_tokens_map.json from cache at /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/hub/models--databricks--dolly-v2-3b/snapshots/f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-23 06:26:12,501 >> loading file tokenizer_config.json from cache at /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/hub/models--databricks--dolly-v2-3b/snapshots/f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-23 06:26:12,666 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using custom data configuration default-23a59859bfe54ec1\n",
      "2024-06-23 06:26:12,961 - builder.py - datasets.builder - INFO - Using custom data configuration default-23a59859bfe54ec1\n",
      "Loading Dataset Infos from /home/u1759c6d9527c9022f99471cd5e84981/.conda/envs/itrex/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "2024-06-23 06:26:12,963 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u1759c6d9527c9022f99471cd5e84981/.conda/envs/itrex/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "2024-06-23 06:26:12,988 - builder.py - datasets.builder - INFO - Generating dataset json (/home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "Downloading and preparing dataset json/default to /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a...\n",
      "2024-06-23 06:26:12,992 - builder.py - datasets.builder - INFO - Downloading and preparing dataset json/default to /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a...\n",
      "Downloading took 0.0 min\n",
      "2024-06-23 06:26:13,000 - download_manager.py - datasets.download.download_manager - INFO - Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "2024-06-23 06:26:13,013 - download_manager.py - datasets.download.download_manager - INFO - Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "2024-06-23 06:26:13,024 - builder.py - datasets.builder - INFO - Generating train split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04144df563fb479b87fe7dc5e97f4932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to verify splits sizes.\n",
      "2024-06-23 06:26:13,242 - info_utils.py - datasets.utils.info_utils - INFO - Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a. Subsequent calls will reuse this data.\n",
      "2024-06-23 06:26:13,252 - builder.py - datasets.builder - INFO - Dataset json downloaded and prepared to /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-23a59859bfe54ec1\n",
      "2024-06-23 06:26:13,547 - builder.py - datasets.builder - INFO - Using custom data configuration default-23a59859bfe54ec1\n",
      "Loading Dataset Infos from /home/u1759c6d9527c9022f99471cd5e84981/.conda/envs/itrex/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "2024-06-23 06:26:13,551 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u1759c6d9527c9022f99471cd5e84981/.conda/envs/itrex/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "2024-06-23 06:26:13,559 - builder.py - datasets.builder - INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "2024-06-23 06:26:13,562 - info.py - datasets.info - INFO - Loading Dataset info from /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "Found cached dataset json (/home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "2024-06-23 06:26:13,572 - builder.py - datasets.builder - INFO - Found cached dataset json (/home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "Loading Dataset info from /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "2024-06-23 06:26:13,575 - info.py - datasets.info - INFO - Loading Dataset info from /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "Using custom data configuration default-23a59859bfe54ec1\n",
      "2024-06-23 06:26:13,851 - builder.py - datasets.builder - INFO - Using custom data configuration default-23a59859bfe54ec1\n",
      "Loading Dataset Infos from /home/u1759c6d9527c9022f99471cd5e84981/.conda/envs/itrex/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "2024-06-23 06:26:13,854 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u1759c6d9527c9022f99471cd5e84981/.conda/envs/itrex/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "2024-06-23 06:26:13,863 - builder.py - datasets.builder - INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "2024-06-23 06:26:13,866 - info.py - datasets.info - INFO - Loading Dataset info from /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "Found cached dataset json (/home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "2024-06-23 06:26:13,875 - builder.py - datasets.builder - INFO - Found cached dataset json (/home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "Loading Dataset info from /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "2024-06-23 06:26:13,877 - info.py - datasets.info - INFO - Loading Dataset info from /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fd7267a29a47749019c7458b8f8d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3474] 2024-06-23 06:26:45,136 >> loading weights file pytorch_model.bin from cache at /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/hub/models--databricks--dolly-v2-3b/snapshots/f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1519] 2024-06-23 06:26:45,717 >> Instantiating GPTNeoXForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-23 06:26:45,722 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4280] 2024-06-23 06:26:46,191 >> All model checkpoint weights were used when initializing GPTNeoXForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-23 06:26:46,192 >> All the weights of GPTNeoXForCausalLM were initialized from the model checkpoint at databricks/dolly-v2-3b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLM for predictions without further training.\n",
      "[INFO|modeling_utils.py:3797] 2024-06-23 06:26:46,304 >> Generation config file not found, using a generation config created from the model config.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5b8ceb5ada41fd9c46c8c756d153fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-3b4498c23a63fd4a.arrow\n",
      "2024-06-23 06:26:47,239 - arrow_dataset.py - datasets.arrow_dataset - INFO - Caching processed dataset at /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-3b4498c23a63fd4a.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284b58f7d3fa4f88acd72a2bf07002b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-373df0f915057236.arrow\n",
      "2024-06-23 06:26:50,019 - arrow_dataset.py - datasets.arrow_dataset - INFO - Caching processed dataset at /home/u1759c6d9527c9022f99471cd5e84981/.cache/huggingface/datasets/json/default-23a59859bfe54ec1/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-373df0f915057236.arrow\n",
      "2024-06-23 06:26:50,040 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - Using data collator of type DataCollatorForSeq2Seq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,621,440 || all params: 2,777,707,520 || trainable%: 0.09437422698844837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:641] 2024-06-23 06:26:50,324 >> Using cpu_amp half precision backend\n",
      "[INFO|trainer.py:2078] 2024-06-23 06:26:50,649 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-23 06:26:50,651 >>   Num examples = 9,900\n",
      "[INFO|trainer.py:2080] 2024-06-23 06:26:50,652 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2081] 2024-06-23 06:26:50,653 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2084] 2024-06-23 06:26:50,654 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-06-23 06:26:50,655 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:2086] 2024-06-23 06:26:50,656 >>   Total optimization steps = 3,711\n",
      "[INFO|trainer.py:2087] 2024-06-23 06:26:50,659 >>   Number of trainable parameters = 2,621,440\n",
      "/home/u1759c6d9527c9022f99471cd5e84981/.conda/envs/itrex/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: aten::layer_norm: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3711' max='3711' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3711/3711 2:41:35, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.587900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.474200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.457300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.420800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2329] 2024-06-23 09:09:08,096 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:3410] 2024-06-23 09:09:08,102 >> Saving model checkpoint to ./tmp\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-23 09:09:08,132 >> tokenizer config file saved in ./tmp/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-23 09:09:08,134 >> Special tokens file saved in ./tmp/special_tokens_map.json\n",
      "2024-06-23 09:09:08,167 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - *** Evaluate After Training***\n",
      "[INFO|trainer.py:3719] 2024-06-23 09:09:08,173 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-06-23 09:09:08,174 >>   Num examples = 100\n",
      "[INFO|trainer.py:3724] 2024-06-23 09:09:08,175 >>   Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =     2.9988\n",
      "  eval_loss               =     0.4564\n",
      "  eval_ppl                =     1.5786\n",
      "  eval_runtime            = 0:00:14.16\n",
      "  eval_samples            =        100\n",
      "  eval_samples_per_second =      7.061\n",
      "  eval_steps_per_second   =      1.765\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary classes from Hugging Face and Intel Extension for Transformers\n",
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "\n",
    "# Setting up model arguments, specifying which pre-trained model to use\n",
    "model_args = ModelArguments(model_name_or_path=\"databricks/dolly-v2-3b\")\n",
    "\n",
    "# Setting up data arguments, including the training file and validation split\n",
    "data_args = DataArguments(train_file=\"alpaca_10000_lines.json\", validation_split_percentage=1)\n",
    "\n",
    "# Configuring training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./tmp1',               # Directory to save model checkpoints\n",
    "    do_train=True,                    # Enable training\n",
    "    do_eval=True,                     # Enable evaluation\n",
    "    num_train_epochs=3,               # Number of training epochs\n",
    "    overwrite_output_dir=True,        # Overwrite content of the output directory\n",
    "    per_device_train_batch_size=4,    # Training batch size per device\n",
    "    per_device_eval_batch_size=4,     # Evaluation batch size per device\n",
    "    gradient_accumulation_steps=2,    # Number of updates steps to accumulate gradients before performing a backward pass\n",
    "    save_strategy=\"no\",               # Strategy for saving checkpoints (here, no intermediate saving)\n",
    "    log_level=\"info\",                 # Logging level\n",
    "    save_total_limit=2,               # Limit the total amount of checkpoints\n",
    "    bf16=True,                        # Use bfloat16 precision for training (if supported)\n",
    ")\n",
    "\n",
    "# Setting up additional finetuning arguments\n",
    "finetune_args = FinetuningArguments()\n",
    "\n",
    "# Combining all the configurations into a single finetuning configuration\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "            model_args=model_args,\n",
    "            data_args=data_args,\n",
    "            training_args=training_args,\n",
    "            finetune_args=finetune_args,\n",
    "        )\n",
    "\n",
    "# Starting the finetuning process with the specified configuration\n",
    "finetune_model(finetune_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging LoRA Adapter into a Language Model\n",
    "\n",
    "This code demonstrates the process of merging a LoRA (Low-Rank Adaptation) adapter into a pre-trained language model (`databricks/dolly-v2-3b`). The merged model is then saved to a specified directory (`./merged_model/`).\n",
    "\n",
    "### What This Code Does:\n",
    "\n",
    "- **Imports Necessary Libraries**:\n",
    "  - Imports essential libraries including `torch` for tensor operations, `AutoModelForCausalLM` and `AutoTokenizer` from Hugging Face's Transformers for model loading and tokenization, and `get_peft_model`, `LoraConfig`, `TaskType` from `peft` for adapter merging and configuration.\n",
    "\n",
    "- **Configuration**:\n",
    "  - Defines the base model name (`databricks/dolly-v2-3b`), the directory containing adapter configuration (`adapter_dir`), and the output directory for the merged model (`merged_model_output_dir`).\n",
    "\n",
    "- **Load the Base Model**:\n",
    "  - Loads the base language model (`AutoModelForCausalLM`) from its pre-trained checkpoint (`base_model_name`) with specific configurations like using `torch.bfloat16` for precision and automatic device mapping (`device_map='auto'`).\n",
    "\n",
    "- **Load the Tokenizer**:\n",
    "  - Loads the tokenizer corresponding to the base model (`AutoTokenizer`), sets `pad_token` to `eos_token`, and configures `padding_side` as \"right\" for token padding.\n",
    "\n",
    "- **Load the LoRA Adapter Configuration**:\n",
    "  - Loads the LoRA adapter configuration (`LoraConfig`) from the specified directory (`adapter_dir`).\n",
    "\n",
    "- **Merge the LoRA Adapter**:\n",
    "  - Utilizes `get_peft_model` function to merge the LoRA adapter configuration (`peft_config`) into the base model (`base_model`), producing `model_with_adapter`.\n",
    "\n",
    "- **Save the Merged Model**:\n",
    "  - Saves the merged model (`model_with_adapter`) and tokenizer (`tokenizer`) to the output directory (`merged_model_output_dir`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Configuration\n",
    "base_model_name = \"databricks/dolly-v2-3b\"\n",
    "adapter_dir = \"./tmp\"  # Directory containing adapter_config.json and adapter_model.bin\n",
    "merged_model_output_dir = \"./merged_model/\"\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load the LoRA adapter configuration\n",
    "peft_config = LoraConfig.from_pretrained(adapter_dir)\n",
    "\n",
    "\n",
    "# Merge the LoRA adapter into the base model\n",
    "model_with_adapter = get_peft_model(base_model, peft_config)\n",
    "\n",
    "# Save the merged model\n",
    "model_with_adapter.save_pretrained(merged_model_output_dir)\n",
    "tokenizer.save_pretrained(merged_model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking the Fine-Tuned Language Model on CPU\n",
    "\n",
    "This code benchmarks the performance of a fine-tuned language model, specifically measuring its average latency and throughput when running on a CPU. The benchmarking is performed using a small portion of the Wikitext-2 dataset.\n",
    "\n",
    "#### What This Code Does:\n",
    "\n",
    "**Imports Necessary Libraries**:\n",
    "- Imports essential libraries and modules for model loading, tokenization, dataset handling, and performance measurement.\n",
    "\n",
    "**Model and Tokenizer Loading**:\n",
    "- Loads the fine-tuned model and tokenizer from the specified directory.\n",
    "\n",
    "**Device Configuration**:\n",
    "- Ensures the model is running on the CPU.\n",
    "\n",
    "**Dataset Loading**:\n",
    "- Loads a small sample from the Wikitext-2 dataset for benchmarking purposes.\n",
    "\n",
    "**Data Preprocessing**:\n",
    "- Preprocesses the dataset to tokenize the text and prepare it for input into the model.\n",
    "\n",
    "**Data Collation**:\n",
    "- Defines a function to collate the data into batches suitable for input into the model.\n",
    "\n",
    "**Benchmarking Function**:\n",
    "- Defines a function to measure the model's average latency and throughput during inference.\n",
    "\n",
    "**Running the Benchmark**:\n",
    "- Runs the benchmark on the CPU and prints the average latency and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name_or_path = './merged_model/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Ensure the model is on CPU\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Load a sample dataset for benchmarking\n",
    "# Using a small dataset for demonstration purposes\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test[:1%]')\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Data collator to handle batching\n",
    "def collate_fn(batch):\n",
    "    return {key: torch.tensor([example[key] for example in batch]) for key in batch[0].keys()}\n",
    "\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=4, collate_fn=collate_fn)\n",
    "\n",
    "# Benchmarking function\n",
    "def benchmark_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {key: value.to(device) for key, value in batch.items()}\n",
    "            start_time = time.time()\n",
    "            outputs = model(**inputs)\n",
    "            total_time += time.time() - start_time\n",
    "            total_tokens += inputs['input_ids'].numel()\n",
    "\n",
    "    avg_latency = total_time / len(dataloader)\n",
    "    throughput = total_tokens / total_time\n",
    "\n",
    "    return avg_latency, throughput\n",
    "\n",
    "# Run benchmark on CPU\n",
    "avg_latency, throughput = benchmark_model(model, dataloader)\n",
    "\n",
    "print(f'Average Latency: {avg_latency:.4f} seconds')\n",
    "print(f'Throughput: {throughput:.2f} tokens/second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below codeblock demonstrates the process of inferencing text using a merged model that combines a pre-trained language model (`databricks/dolly-v2-3b`) with a LoRA (Low-Rank Adaptation) adapter.\n",
    "\n",
    "### Overview:\n",
    "1. **Configuration**:\n",
    "    - Defines the base model name (`databricks/dolly-v2-3b`), the directory containing the adapter configuration (`adapter_dir`), and the output directory for the merged model (`merged_model_output_dir`).\n",
    "\n",
    "2. **Load Tokenizer and Model**:\n",
    "    - Loads the tokenizer and the merged model from the specified output directory.\n",
    "\n",
    "3. **System Message and Prompt**:\n",
    "    - Sets a system message guiding the model to generate engaging and positive stories.\n",
    "    - Defines a user prompt to initiate the story generation.\n",
    "\n",
    "4. **Tokenization**:\n",
    "    - Tokenizes the combined system message and user prompt.\n",
    "\n",
    "5. **Model Evaluation and Text Generation**:\n",
    "    - Ensures the model is in evaluation mode.\n",
    "    - Generates text token-by-token up to a specified maximum length (`max_length`), printing each token as it is generated.\n",
    "    - Times the generation process to measure performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/u1759c6d9527c9022f99471cd5e84981/.conda/envs/itrex/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "# Assuming these are your configurations\n",
    "base_model_name = \"databricks/dolly-v2-3b\"\n",
    "adapter_dir = \"./tmp\"  # Directory containing adapter_config.json and adapter_model.bin\n",
    "merged_model_output_dir = \"./merged_model/\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_model_output_dir)\n",
    "\n",
    "# Load the merged model\n",
    "model = AutoModelForCausalLM.from_pretrained(merged_model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Engineering is a broad field that includes many different disciplines. Some of the more popular computer engineering disciplines include computer architecture, computer engineering, computer graphics, and computer science. Computer engineering is the study of the design, construction, and operation of electronic systems and devices. Computer engineering students typically focus on the design of hardware and software, and the integration of these components into larger systems.\n",
      "\n",
      "\n",
      "\n",
      "Time taken for generation: 19.80 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the system message for story generation\n",
    "system_message = (\n",
    "    \"You are a creative and imaginative storyteller. Your task is to continue stories in a captivating and coherent manner. \"\n",
    "    \"Ensure that your narratives are engaging, appropriate for all audiences, and maintain a positive tone. Avoid any content that is harmful, unethical, racist, sexist, toxic, dangerous, or illegal. \"\n",
    "    \"Strive to create stories that are socially unbiased and enjoyable. If a prompt is unclear or does not make sense, provide a creative and sensible continuation while maintaining coherence.\\n\"\n",
    ")\n",
    "\n",
    "# Example prompt for generation\n",
    "user_prompt = \"Computer Engineering is\"\n",
    "prompt = system_message + user_prompt\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure that the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate tokens step-by-step\n",
    "max_length = 75\n",
    "generated_tokens = inputs[\"input_ids\"]\n",
    "generated_text = \"\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "print(user_prompt, end='', flush=True)  # Print the user prompt\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_length):\n",
    "        outputs = model(generated_tokens)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Get the most probable next token\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Append the new token to the generated sequence\n",
    "        generated_tokens = torch.cat((generated_tokens, next_token_id), dim=-1)\n",
    "\n",
    "        # Decode and print the generated token\n",
    "        generated_text_chunk = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "        generated_text += generated_text_chunk\n",
    "        print(generated_text_chunk, end='', flush=True)  # Print each token as it's generated\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print()  # Ensure the output ends with a newline\n",
    "print(f\"\\nTime taken for generation: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl who loved to read. She loved to read books about princesses, fairies, and unicorns. She loved to read about the world she created in her head. She loved to read until one day, she found a book that changed her life. It was a book about a little girl who loved to read. The book was called\n",
      "\n",
      "Time taken for generation: 19.83 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the system message for story generation\n",
    "system_message = (\n",
    "    \"You are a creative and imaginative storyteller. Your task is to continue stories in a captivating and coherent manner. \"\n",
    "    \"Ensure that your narratives are engaging, appropriate for all audiences, and maintain a positive tone. Avoid any content that is harmful, unethical, racist, sexist, toxic, dangerous, or illegal. \"\n",
    "    \"Strive to create stories that are socially unbiased and enjoyable. If a prompt is unclear or does not make sense, provide a creative and sensible continuation while maintaining coherence.\\n\"\n",
    ")\n",
    "\n",
    "# Example prompt for generation\n",
    "user_prompt = \"Once upon a time\"\n",
    "prompt = system_message + user_prompt\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure that the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate tokens step-by-step\n",
    "max_length = 75\n",
    "generated_tokens = inputs[\"input_ids\"]\n",
    "generated_text = \"\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "print(user_prompt, end='', flush=True)  # Print the user prompt\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_length):\n",
    "        outputs = model(generated_tokens)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Get the most probable next token\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Append the new token to the generated sequence\n",
    "        generated_tokens = torch.cat((generated_tokens, next_token_id), dim=-1)\n",
    "\n",
    "        # Decode and print the generated token\n",
    "        generated_text_chunk = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "        generated_text += generated_text_chunk\n",
    "        print(generated_text_chunk, end='', flush=True)  # Print each token as it's generated\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print()  # Ensure the output ends with a newline\n",
    "print(f\"\\nTime taken for generation: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long ago lived a king and queen. They had three children: a boy and two girls. The boy was very handsome and the girls were beautiful. The king and queen loved their children very much. They were so proud of them. The king and queen were especially proud of their son. He was a great warrior and a hero. He was strong and brave. He was a true leader. He\n",
      "\n",
      "Time taken for generation: 20.96 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the system message for story generation\n",
    "system_message = (\n",
    "    \"You are a creative and imaginative storyteller. Your task is to continue stories in a captivating and coherent manner. \"\n",
    "    \"Ensure that your narratives are engaging, appropriate for all audiences, and maintain a positive tone. Avoid any content that is harmful, unethical, racist, sexist, toxic, dangerous, or illegal. \"\n",
    "    \"Strive to create stories that are socially unbiased and enjoyable. If a prompt is unclear or does not make sense, provide a creative and sensible continuation while maintaining coherence.\\n\"\n",
    ")\n",
    "\n",
    "# Example prompt for generation\n",
    "user_prompt = \"Long ago lived a king\"\n",
    "prompt = system_message + user_prompt\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure that the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate tokens step-by-step\n",
    "max_length = 75\n",
    "generated_tokens = inputs[\"input_ids\"]\n",
    "generated_text = \"\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "print(user_prompt, end='', flush=True)  # Print the user prompt\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_length):\n",
    "        outputs = model(generated_tokens)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Get the most probable next token\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Append the new token to the generated sequence\n",
    "        generated_tokens = torch.cat((generated_tokens, next_token_id), dim=-1)\n",
    "\n",
    "        # Decode and print the generated token\n",
    "        generated_text_chunk = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "        generated_text += generated_text_chunk\n",
    "        print(generated_text_chunk, end='', flush=True)  # Print each token as it's generated\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print()  # Ensure the output ends with a newline\n",
    "print(f\"\\nTime taken for generation: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a faraway kingdom, there lived a princess named Snow White. She was as beautiful as a blooming rose, with skin as white as snow, lips as red as blood, and hair as black as ebony. Her mother, the queen, passed away when Snow White was very young, and her father, the king, remarried a vain and wicked woman who was obsessed with her own beauty. The new queen owned a magical mirror that she would often consult, asking who is the fairest of them all. The mirror always replied that she, the queen, was the fairest of them all. However, as Snow White grew older, her beauty surpassed that of the queen, and one day, when the queen asked her mirror the familiar question, it replied that Snow White is the fairest of them all. Consumed with jealousy, the queen ordered a huntsman to take Snow White into the forest and kill her. She demanded that he bring back Snow White's heart as proof. The huntsman, however, could not bring himself to harm the innocent princess. Instead, he let her go, advising her to run far away and never return. To deceive the queen, he brought back the heart of a wild boar. Snow White fled to a nearby castle, where she was welcomed by the king and his daughter, the princesses. The king and princesses loved Snow White dearly, and they decided to hide her from the queen. They dressed her up in beautiful clothes and made her their own personal lady-in-waiting. The queen, however, did not give up. She\n",
      "\n",
      "Time taken for generation: 59.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the system message for story generation\n",
    "system_message = (\n",
    "    \"You are a creative and imaginative storyteller. Your task is to continue stories in a captivating and coherent manner. \"\n",
    "    \"Ensure that your narratives are engaging, appropriate for all audiences, and maintain a positive tone. Avoid any content that is harmful, unethical, racist, sexist, toxic, dangerous, or illegal. \"\n",
    "    \"Strive to create stories that are socially unbiased and enjoyable. If a prompt is unclear or does not make sense, provide a creative and sensible continuation while maintaining coherence.\\n\"\n",
    ")\n",
    "\n",
    "# Example prompt for generation\n",
    "user_prompt = \"Once upon a time, in a faraway kingdom, there lived a princess named Snow White. She was as beautiful as a blooming rose, with skin as white as snow, lips as red as blood, and hair as black as ebony. Her mother, the queen, passed away when Snow White was very young, and her father, the king, remarried a vain and wicked woman who was obsessed with her own beauty. The new queen owned a magical mirror that she would often consult, asking who is the fairest of them all. The mirror always replied that she, the queen, was the fairest of them all. However, as Snow White grew older, her beauty surpassed that of the queen, and one day, when the queen asked her mirror the familiar question, it replied that Snow White is the fairest of them all. Consumed with jealousy, the queen ordered a huntsman to take Snow White into the forest and kill her. She demanded that he bring back Snow White's heart as proof. The huntsman, however, could not bring himself to harm the innocent princess. Instead, he let her go, advising her to run far away and never return. To deceive the queen, he brought back the heart of a wild boar.\"\n",
    "prompt = system_message + user_prompt\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure that the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate tokens step-by-step\n",
    "max_length = 75\n",
    "generated_tokens = inputs[\"input_ids\"]\n",
    "generated_text = \"\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "print(user_prompt, end='', flush=True)  # Print the user prompt\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_length):\n",
    "        outputs = model(generated_tokens)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Get the most probable next token\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Append the new token to the generated sequence\n",
    "        generated_tokens = torch.cat((generated_tokens, next_token_id), dim=-1)\n",
    "\n",
    "        # Decode and print the generated token\n",
    "        generated_text_chunk = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "        generated_text += generated_text_chunk\n",
    "        print(generated_text_chunk, end='', flush=True)  # Print each token as it's generated\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print()  # Ensure the output ends with a newline\n",
    "print(f\"\\nTime taken for generation: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in Stanford's English classroom, there was a young man named Harry Potter. He was a very smart wizard, and he was very good at his job. He was also a very bad boy. He would do things like drink excessively, get into fights, and generally cause trouble. One day, Harry Potter was walking down the street when he saw a beautiful girl. He was immediately attracted\n",
      "\n",
      "Time taken for generation: 18.86 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the system message for story generation\n",
    "system_message = (\n",
    "    \"You are a creative and imaginative storyteller. Your task is to continue stories in a captivating and coherent manner. \"\n",
    "    \"Ensure that your narratives are engaging, appropriate for all audiences, and maintain a positive tone. Avoid any content that is harmful, unethical, racist, sexist, toxic, dangerous, or illegal. \"\n",
    "    \"Strive to create stories that are socially unbiased and enjoyable. If a prompt is unclear or does not make sense, provide a creative and sensible continuation while maintaining coherence.\\n\"\n",
    ")\n",
    "\n",
    "# Example prompt for generation\n",
    "user_prompt = \"Once upon a time in Stanford\"\n",
    "prompt = system_message + user_prompt\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure that the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate tokens step-by-step\n",
    "max_length = 75\n",
    "generated_tokens = inputs[\"input_ids\"]\n",
    "generated_text = \"\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "print(user_prompt, end='', flush=True)  # Print the user prompt\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_length):\n",
    "        outputs = model(generated_tokens)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        # Get the most probable next token\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # Append the new token to the generated sequence\n",
    "        generated_tokens = torch.cat((generated_tokens, next_token_id), dim=-1)\n",
    "\n",
    "        # Decode and print the generated token\n",
    "        generated_text_chunk = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "        generated_text += generated_text_chunk\n",
    "        print(generated_text_chunk, end='', flush=True)  # Print each token as it's generated\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print()  # Ensure the output ends with a newline\n",
    "print(f\"\\nTime taken for generation: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-chat",
   "language": "python",
   "name": "neural-chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
