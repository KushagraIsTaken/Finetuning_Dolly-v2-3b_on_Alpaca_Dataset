@article{Brown2020,
   abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
   author = {Tom B Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam Mccandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
   journal = {Advances in Neural Information Processing Systems},
   pages = {1877-1901},
   title = {Language Models are Few-Shot Learners},
   volume = {33},
   url = {https://commoncrawl.org/the-data/},
   year = {2020},
}
@article{Peters2018,
   abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-Trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
   author = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
   doi = {10.18653/V1/N18-1202},
   isbn = {9781948087278},
   journal = {NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
   pages = {2227-2237},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Deep Contextualized Word Representations},
   volume = {1},
   url = {https://aclanthology.org/N18-1202},
   year = {2018},
}
@article{Howard2018,
   abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100× more data. We open-source our pretrained models and code1},
   author = {Jeremy Howard and Sebastian Ruder},
   doi = {10.18653/V1/P18-1031},
   isbn = {9781948087322},
   journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
   pages = {328-339},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Universal Language Model Fine-tuning for Text Classification},
   volume = {1},
   url = {https://aclanthology.org/P18-1031},
   year = {2018},
}
@article{Raffel2020,
   abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code. 1},
   author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
   issn = {1533-7928},
   issue = {140},
   journal = {Journal of Machine Learning Research},
   keywords = {attention-based models,deep learning,multi-task learning,natural language processing,transfer learning},
   pages = {1-67},
   title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
   volume = {21},
   url = {http://jmlr.org/papers/v21/20-074.html},
   year = {2020},
}
@article{Lee2020,
   abstract = {Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results: We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.},
   author = {Jinhyuk Lee and Wonjin Yoon and Sungdong Kim and Donghyeon Kim and Sunkyu Kim and Chan Ho So and Jaewoo Kang},
   doi = {10.1093/BIOINFORMATICS/BTZ682},
   issn = {1367-4803},
   issue = {4},
   journal = {Bioinformatics},
   month = {2},
   pages = {1234-1240},
   pmid = {31501885},
   publisher = {Oxford Academic},
   title = {BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
   volume = {36},
   url = {https://dx.doi.org/10.1093/bioinformatics/btz682},
   year = {2020},
}
@article{Gururangan2020,
   abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.},
   author = {Suchin Gururangan and Ana Marasovic and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
   doi = {10.18653/V1/2020.ACL-MAIN.740},
   isbn = {9781952148255},
   issn = {0736587X},
   journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
   pages = {8342-8360},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks},
   url = {https://aclanthology.org/2020.acl-main.740},
   year = {2020},
}
@article{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
   author = {Ashish Vaswani and Google Brain and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Łukasz Kaiser and Illia Polosukhin},
   journal = {Advances in Neural Information Processing Systems},
   title = {Attention is All you Need},
   volume = {30},
   year = {2017},
}
@report{,
   abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
   author = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
   title = {Language Models are Unsupervised Multitask Learners},
   url = {https://github.com/codelucas/newspaper},
}
@article{Devlin2019,
   abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Rad-ford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result , the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
   author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova Google and A I Language},
   city = {Stroudsburg, PA, USA},
   doi = {10.18653/V1/N19-1423},
   journal = {Proceedings of the 2019 Conference of the North},
   pages = {4171-4186},
   publisher = {Association for Computational Linguistics},
   title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
   url = {https://aclanthology.org/N19-1423},
   year = {2019},
}
@article{,
   abstract = {Large language models (LLMs) represent a major advance in artificial intelligence and, in particular, toward the goal of human-like artificial general intelligence. It is sometimes claimed, though, that machine learning is “just statistics,” hence that, in this grander ambition, progress in AI is illusory. Here I take the contrary view that LLMs have a great deal to teach us about the nature of language, understanding, intelligence, sociality, and personhood. Specifically: statistics do amount to understanding, in any falsifiable sense. Furthermore, much of what we consider intelligence is inherently dialogic, hence social; it requires a theory of mind. Complex sequence learning and social interaction may be a sufficient basis for general intelligence, including theory of mind and consciousness. Since the interior state of another being can only be understood through interaction, no objective answer is possible to the question of when an “it” becomes a “who,” but for many people, neural nets running on computers are likely to cross this threshold in the very near future.},
   author = {Blaise Agüera Y Arcas},
   doi = {10.1162/DAED_A_01909},
   issn = {0011-5266},
   issue = {2},
   journal = {Daedalus},
   month = {5},
   pages = {183-197},
   publisher = {MIT Press},
   title = {Do Large Language Models Understand Us?},
   volume = {151},
   url = {https://dx.doi.org/10.1162/daed_a_01909},
   year = {2022},
}
@article{Makridakis2023,
   abstract = {ChatGPT, a state-of-the-art large language model (LLM), is revolutionizing the AI field by exhibiting humanlike skills in a range of tasks that include understanding and answering natural language questions, translating languages, writing code, passing professional exams, and even composing poetry, among its other abilities. ChatGPT has gained an immense popularity since its launch, amassing 100 million active monthly users in just two months, thereby establishing itself as the fastest-growing consumer application to date. This paper discusses the reasons for its success as well as the future prospects of similar large language models (LLMs), with an emphasis on their potential impact on forecasting, a specialized and domain-specific field. This is achieved by first comparing the correctness of the answers of the standard ChatGPT and a custom one, trained using published papers from a subfield of forecasting where the answers to the questions asked are known, allowing us to determine their correctness compared to those of the two ChatGPT versions. Then, we also compare the responses of the two versions on how judgmental adjustments to the statistical/ML forecasts should be applied by firms to improve their accuracy. The paper concludes by considering the future of LLMs and their impact on all aspects of our life and work, as well as on the field of forecasting specifically. Finally, the conclusion section is generated by ChatGPT, which was provided with a condensed version of this paper and asked to write a four-paragraph conclusion.},
   author = {Spyros Makridakis and Fotios Petropoulos and Yanfei Kang},
   doi = {10.3390/FORECAST5030030},
   issn = {2571-9394},
   issue = {3},
   journal = {Forecasting 2023, Vol. 5, Pages 536-549},
   keywords = {ChatGPT,Forecasting,Large Language Models},
   month = {8},
   pages = {536-549},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Large Language Models: Their Success and Impact},
   volume = {5},
   url = {https://www.mdpi.com/2571-9394/5/3/30/htm https://www.mdpi.com/2571-9394/5/3/30},
   year = {2023},
}
@article{Shen2018,
   abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F\{0\} features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.},
   author = {Jonathan Shen and Ruoming Pang and Ron J. Weiss and Mike Schuster and Navdeep Jaitly and Zongheng Yang and Zhifeng Chen and Yu Zhang and Yuxuan Wang and Rj Skerrv-Ryan and Rif A. Saurous and Yannis Agiomvrgiannakis and Yonghui Wu},
   doi = {10.1109/ICASSP.2018.8461368},
   isbn = {9781538646588},
   issn = {15206149},
   journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
   keywords = {Tacotron 2,Text-to-speech,WaveNet},
   month = {9},
   pages = {4779-4783},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions},
   volume = {2018-April},
   year = {2018},
}
@report{,
   abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, GLIDE and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.},
   author = {Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and S Sara Mahdavi and Raphael Gontijo-Lopes and Tim Salimans and Jonathan Ho and David J Fleet and Mohammad Norouzi},
   title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
}
@article{Zhu2017,
   abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
   author = {Jun Yan Zhu and Taesung Park and Phillip Isola and Alexei A. Efros},
   doi = {10.1109/ICCV.2017.244},
   isbn = {9781538610329},
   issn = {15505499},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   month = {12},
   pages = {2242-2251},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks},
   volume = {2017-October},
   year = {2017},
}
@article{Gu2021,
   abstract = {Recent advances in pre-trained language models have significantly improved neural response generation. However, existing methods usually view the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level self-attention. Such token-level encoding hinders the exploration of discourse-level coherence among utterances. This paper presents DialogBERT, a novel conversational response generation model that enhances previous PLM-based dialogue models. DialogBERT employs a hierarchical Transformer architecture. To efficiently capture the discourse-level coherence among utterances, we propose two training objectives, including masked utterance regression and distributed utterance order ranking in analogy to the original BERT training. Experiments on three multi-turn conversation datasets show that our approach remarkably outperforms three baselines, such as BART and DialoGPT, in terms of quantitative evaluation. The human evaluation suggests that DialogBERT generates more coherent, informative, and human-like responses than the baselines with significant margins.},
   author = {Xiaodong Gu and Kang Min Yoo and Jung Woo Ha},
   doi = {10.1609/AAAI.V35I14.17527},
   isbn = {9781713835974},
   issn = {2374-3468},
   issue = {14},
   journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
   keywords = {Conversational AI/Dialog Systems},
   month = {5},
   pages = {12911-12919},
   publisher = {Association for the Advancement of Artificial Intelligence},
   title = {DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances},
   volume = {35},
   url = {https://ojs.aaai.org/index.php/AAAI/article/view/17527},
   year = {2021},
}
@article{Mo2019,
   abstract = {The ability to generate novel, diverse, and realistic 3D shapes along with associated part semantics and structure is central to many applications requiring high-quality 3D assets or large volumes of realistic training data. A key challenge towards this goal is how to accommodate diverse shape variations, including both continuous deformations of parts as well as structural or discrete alterations which add to, remove from, or modify the shape constituents and compositional structure. Such object structure can typically be organized into a hierarchy of constituent object parts and relationships, represented as a hierarchy of n-ary graphs. We introduce StructureNet, a hierarchical graph network which (i) can directly encode shapes represented as such n-ary graphs, (ii) can be robustly trained on large and complex shape families, and (iii) be used to generate a great diversity of realistic structured shape geometries. Technically, we accomplish this by drawing inspiration from recent advances in graph neural networks to propose an order-invariant encoding of n-ary graphs, considering jointly both part geometry and inter-part relations during network training. We extensively evaluate the quality of the learned latent spaces for various shape families and show significant advantages over baseline and competing methods. The learned latent spaces enable several structure-aware geometry processing applications, including shape generation and interpolation, shape editing, or shape structure discovery directly from un-annotated images, point clouds, or partial scans.},
   author = {Kaichun Mo and Paul Guerrero and Li Yi and Hao Su and Peter Wonka and Kaust Niloy J. Mitra and Leonidas J. Guibas},
   doi = {10.1145/3355089.3356527/SUPPL_FILE/A242-MO.ZIP},
   issn = {15577368},
   issue = {6},
   journal = {ACM Transactions on Graphics},
   keywords = {Autoencoder,Generative models,Graph neural networks,Object structure,Shape analysis and synthesis},
   month = {11},
   publisher = {Association for Computing Machinery},
   title = {StructureNet: Hierarchical graph networks for 3D shape generation},
   volume = {38},
   url = {https://dl.acm.org/doi/10.1145/3355089.3356527},
   year = {2019},
}
@inproceedings{,
   author = {Aäron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alex Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu},
   journal = {Proc. 9th ISCA Workshop on Speech Synthesis Workshop (SSW 9)},
   pages = {125},
   title = {WaveNet: A Generative Model for Raw Audio},
   year = {2016},
}
@article{Bilokopytov2022,
   abstract = {In this article we investigate disjointly non-singular (DNS) operators. Following [9] we say that an operator T from a Banach lattice F into a Banach space E is DNS, if no restriction of T to a subspace generated by a disjoint sequence is strictly singular. We partially answer a question from [9] by showing that this class of operators forms an open subset of L(F,E) as soon as F is order continuous. Moreover, we show that in this case T is DNS if and only if the norm topology is the minimal topology which is simultaneously stronger than the unbounded norm topology and the topology generated by T as a map (we say that T “complements” the unbounded norm topology in F). Since the class of DNS operators plays a similar role in the category of Banach lattices as the upper semi-Fredholm operators play in the category of Banach spaces, we investigate and indeed uncover a similar characterization of the latter class of operators, but this time they have to complement the weak topology.},
   author = {Eugene Bilokopytov},
   doi = {10.1016/J.JMAA.2021.125556},
   issn = {0022-247X},
   issue = {1},
   journal = {Journal of Mathematical Analysis and Applications},
   keywords = {Banach lattices,Disjointly non-singular operators,Upper semi-Fredholm operators},
   month = {2},
   pages = {125556},
   publisher = {Academic Press},
   title = {Disjointly non-singular operators on order continuous Banach lattices complement the unbounded norm topology},
   volume = {506},
   year = {2022},
}
@web_page{,
   title = {MuseNet | OpenAI},
   url = {https://openai.com/index/musenet/},
}
@report{,
   abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Rad-ford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result , the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
   author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova Google and A I Language},
   pages = {4171-4186},
   title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
   url = {https://github.com/tensorflow/tensor2tensor},
}
@article{Karras2021,
   abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
   author = {Tero Karras and Samuli Laine and Timo Aila},
   doi = {10.1109/TPAMI.2020.2970919},
   issn = {0162-8828},
   issue = {12},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {Aerospace Electronics,Deep Learning,Generative Adversarial Networks,Generative Models,Image Resolution,Interpolation,Natural Languages,Navigation,Neural Networks,Training Data,Visualization},
   month = {12},
   pages = {4217-4228},
   pmid = {32012000},
   publisher = {IEEE Computer Society},
   title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
   volume = {43},
   year = {2021},
}
@report{,
   abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
   author = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
   title = {Language Models are Unsupervised Multitask Learners},
   url = {https://github.com/codelucas/newspaper},
}
@article{Yu2023,
   abstract = {Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.},
   author = {Ping Yu and Hua Xu and Xia Hu and Chao Deng},
   doi = {10.3390/HEALTHCARE11202776},
   issn = {2227-9032},
   issue = {20},
   journal = {Healthcare 2023, Vol. 11, Page 2776},
   keywords = {LLM,ethics,generative AI,generative artificial intelligence,healthcare,large language models,medicine},
   month = {10},
   pages = {2776},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration},
   volume = {11},
   url = {https://www.mdpi.com/2227-9032/11/20/2776/htm https://www.mdpi.com/2227-9032/11/20/2776},
   year = {2023},
}
@article{Huang2023,
   abstract = {Fine-tuning is the most effective way of adapting pre-trained large language models (LLMs) to downstream applications. With the fast growth of LLM-enabled AI applications and democratization of open-souced LLMs, fine-tuning has become possible for non-expert individuals, but intensively performed LLM fine-tuning worldwide could result in significantly high energy consumption and carbon footprint, which may bring large environmental impact. Mitigating such environmental impact towards Green AI directly correlates to reducing the FLOPs of fine-tuning, but existing techniques on efficient LLM fine-tuning can only achieve limited reduction of such FLOPs, due to their ignorance of the backpropagation cost in fine-tuning. To address this limitation, in this paper we present GreenTrainer, a new LLM fine-tuning technique that adaptively evaluates different tensors' backpropagation costs and contributions to the fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the most appropriate set of tensors in training. Such selection in GreenTrainer is made based on a given objective of FLOPs reduction, which can flexibly adapt to the carbon footprint in energy supply and the need in Green AI. Experiment results over multiple open-sourced LLM models and abstractive summarization datasets show that, compared to fine-tuning the whole LLM model, GreenTrainer can save up to 64% FLOPs in fine-tuning without any noticeable model accuracy loss. Compared to the existing fine-tuning techniques such as LoRa, GreenTrainer can achieve up to 4% improvement on model accuracy with on-par FLOPs reduction.},
   author = {Kai Huang and Hanyun Yin and Heng Huang and Wei Gao},
   month = {9},
   title = {Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation},
   url = {https://arxiv.org/abs/2309.13192v2},
   year = {2023},
}
@generic{Taori2023,
   author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B Hashimoto},
   journal = {GitHub repository},
   publisher = {GitHub},
   title = {Stanford Alpaca: An Instruction-following LLaMA model},
   year = {2023},
}
@generic{Conover2023,
   author = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
   title = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
   url = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
   year = {2023},
}
@article{Frantar2023,
   author = {Elias Frantar and Dan Alistarh},
   doi = {10.48550/ARXIV.2301.00774},
   journal = {International Conference on Machine Learning},
   title = {SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},
   year = {2023},
}
@article{Brown2020,
   abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
   author = {Tom B Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam Mccandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
   journal = {Advances in Neural Information Processing Systems},
   pages = {1877-1901},
   title = {Language Models are Few-Shot Learners},
   volume = {33},
   url = {https://commoncrawl.org/the-data/},
   year = {2020},
}
